{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi search agent Rag \n",
    "\n",
    "* Arxiv  Wikipedia  pdf \n",
    "\n",
    "This project implements a multi-search agent using Retrieval-Augmented Generation (RAG) to query and retrieve information from multiple sources, including Wikipedia, PDF documents, and ArXiv. The system also includes a chatbot interface for user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "API_Hugging = os.getenv('HUGGING_API')\n",
    "\n",
    "Gorq_Api = os.getenv('GORQ_API')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_Wrapper = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=300)\n",
    "Wiki_tool = WikipediaQueryRun(api_wrapper=wikipedia_Wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(api_key=API_Hugging,model_name=\"sentence-transformers/all-MiniLM-l6-v2\")\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb=FAISS.from_documents(documents,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInferenceAPIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x72e1e4772410>, search_kwargs={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retriever function in LangChain is typically used to fetch relevant information or documents from a dataset, database, or vector store based on a user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(vectordb,\n",
    "                                     name=\"information about deep learing\",\n",
    "                                     description='Search inforations about Machine learing and deep learing .you must use this tool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arxiv Tool\n",
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "arxiv_wapper = ArxivAPIWrapper(\n",
    "    top_k_results = 2,\n",
    "    ARXIV_MAX_QUERY_LENGTH = 300,\n",
    "    load_max_docs = 3,\n",
    "    load_all_available_meta = False,\n",
    "    doc_content_chars_max = 200\n",
    ")\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combain \n",
    "tools =[arxiv,Wiki_tool,retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm model \n",
    "from langchain_groq import ChatGroq\n",
    "chat = ChatGroq(temperature=0, groq_api_key=Gorq_Api, model_name=\"deepseek-r1-distill-llama-70b\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenerativeAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
